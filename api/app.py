import os
import psycopg2
import datetime
import nltk
import praw
import pickle
import numpy as np
from flask import Flask, render_template, jsonify, request
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import yfinance as yf
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

# --- NLTK SETUP ---
nltk.download('vader_lexicon', download_dir='/tmp/nltk_data')
NLTK_DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'nltk_data')
if NLTK_DATA_PATH not in nltk.data.path:
    nltk.data.path.append(NLTK_DATA_PATH)

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# --- APP CONFIGURATION ---
app = Flask(__name__)

# --- FINBERT SETUP (Advanced NLP) ---
try:
    finbert_tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
    finbert_model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
    USE_FINBERT = True
    print("‚úÖ FinBERT loaded successfully")
except Exception as e:
    print(f"‚ö†Ô∏è FinBERT loading failed, falling back to VADER: {e}")
    USE_FINBERT = False

# --- REDDIT API SETUP ---
try:
    reddit = praw.Reddit(
        client_id=os.environ.get("REDDIT_CLIENT_ID"),
        client_secret=os.environ.get("REDDIT_CLIENT_SECRET"),
        user_agent="SynapseSentimentEngine/1.0"
    )
    USE_REDDIT = True
    print("‚úÖ Reddit API connected")
except Exception as e:
    print(f"‚ö†Ô∏è Reddit API failed, using fallback data: {e}")
    USE_REDDIT = False

# --- ML MODEL SETUP ---
MODEL_PATH = 'sentiment_predictor.pkl'
SCALER_PATH = 'scaler.pkl'

def load_or_create_model():
    """Load existing ML model or create a new one"""
    if os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH):
        with open(MODEL_PATH, 'rb') as f:
            model = pickle.load(f)
        with open(SCALER_PATH, 'rb') as f:
            scaler = pickle.load(f)
        print("‚úÖ ML Model loaded from disk")
        return model, scaler
    else:
        # Create new model with default parameters
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        scaler = StandardScaler()
        print("‚ö†Ô∏è New ML model created (needs training)")
        return model, scaler

ml_model, ml_scaler = load_or_create_model()

# --- DATABASE CONNECTION ---
def get_db_connection():
    """Connects to Vercel Postgres using the environment variable."""
    try:
        conn = psycopg2.connect(os.environ["POSTGRES_URL"])
        return conn
    except Exception as e:
        print(f"‚ùå Database Connection Failed: {e}")
        return None

def init_db():
    """Creates the table if it doesn't exist."""
    conn = get_db_connection()
    if conn:
        cur = conn.cursor()
        cur.execute('''CREATE TABLE IF NOT EXISTS logs (
                        id SERIAL PRIMARY KEY,
                        ticker TEXT,
                        source TEXT,
                        text TEXT,
                        score REAL,
                        label TEXT,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )''')
        # Add table for ML training data
        cur.execute('''CREATE TABLE IF NOT EXISTS training_data (
                        id SERIAL PRIMARY KEY,
                        ticker TEXT,
                        sentiment_score REAL,
                        price_change REAL,
                        volume BIGINT,
                        label TEXT,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )''')
        conn.commit()
        cur.close()
        conn.close()

# --- SENTIMENT ANALYSIS FUNCTIONS ---
def analyze_with_finbert(text):
    """Advanced sentiment analysis using FinBERT"""
    try:
        inputs = finbert_tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        outputs = finbert_model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        # FinBERT outputs: [positive, negative, neutral]
        sentiment_score = probs[0][0].item() - probs[0][1].item()  # positive - negative
        return sentiment_score
    except Exception as e:
        print(f"FinBERT error: {e}")
        return sia.polarity_scores(text)['compound']

def analyze_sentiment(text):
    """Route to appropriate sentiment analyzer"""
    if USE_FINBERT:
        return analyze_with_finbert(text)
    else:
        return sia.polarity_scores(text)['compound']

# --- DATA COLLECTION FUNCTIONS ---
def fetch_reddit_posts(ticker, limit=50):
    """Fetch real posts from Reddit about the ticker"""
    posts = []
    try:
        subreddits = ['wallstreetbets', 'stocks', 'investing', 'StockMarket']
        for subreddit_name in subreddits:
            subreddit = reddit.subreddit(subreddit_name)
            # Search for ticker mentions
            for submission in subreddit.search(f"${ticker} OR {ticker}", limit=limit//len(subreddits), time_filter='day'):
                posts.append({
                    'text': f"{submission.title}. {submission.selftext[:200]}",
                    'source': f'Reddit: r/{subreddit_name}',
                    'score': submission.score,
                    'created': submission.created_utc
                })
            # Also check hot posts for mentions
            for submission in subreddit.hot(limit=10):
                if ticker.upper() in submission.title.upper() or f"${ticker}" in submission.selftext:
                    posts.append({
                        'text': f"{submission.title}. {submission.selftext[:200]}",
                        'source': f'Reddit: r/{subreddit_name}',
                        'score': submission.score,
                        'created': submission.created_utc
                    })
    except Exception as e:
        print(f"Reddit fetch error: {e}")
    
    return posts if posts else None

def fetch_social_data(ticker):
    """
    Fetch social media data - uses real Reddit API or falls back to simulation
    """
    if USE_REDDIT:
        reddit_posts = fetch_reddit_posts(ticker)
        if reddit_posts:
            return [(post['text'], post['source']) for post in reddit_posts[:20]]
    
    # Fallback: Simulation data (for demo purposes)
    import random
    templates = [
        (f"${ticker} is breaking out! üöÄ Huge volume incoming.", "Twitter"),
        (f"I'm worried about ${ticker}'s upcoming earnings report.", "Reddit: r/stocks"),
        (f"Just loaded up on more ${ticker} shares. Long term hold.", "Reddit: r/investing"),
        (f"Market is red, but ${ticker} is holding strong. Bullish sign.", "Twitter"),
        (f"Technical analysis shows a double top for ${ticker}. Selling my position.", "Reddit: r/wallstreetbets"),
        (f"${ticker} to the moon! üåï Who's with me?", "Twitter"),
        (f"Analysts just upgraded ${ticker} to buy. Finally!", "Reddit: r/StockMarket"),
        (f"${ticker} earnings beat expectations. This is huge.", "Twitter"),
        (f"Concerned about ${ticker}'s debt levels. Anyone else?", "Reddit: r/stocks"),
        (f"${ticker} forming a perfect cup and handle pattern.", "Twitter")
    ]
    return [random.choice(templates) for _ in range(15)]

# --- ML PREDICTION FUNCTIONS ---
def get_ml_prediction(ticker, sentiment_scores):
    """Use ML model to predict price movement based on sentiment and other features"""
    try:
        # Get recent price data for features
        stock = yf.Ticker(ticker)
        hist = stock.history(period="5d")
        
        if len(hist) < 2:
            return None, "Insufficient data"
        
        # Calculate features
        avg_sentiment = np.mean(sentiment_scores)
        sentiment_std = np.std(sentiment_scores)
        recent_price_change = (hist['Close'].iloc[-1] - hist['Close'].iloc[-2]) / hist['Close'].iloc[-2]
        avg_volume = hist['Volume'].mean()
        
        # Create feature vector
        features = np.array([[avg_sentiment, sentiment_std, recent_price_change, avg_volume]])
        
        # Scale features (if model has been trained)
        try:
            features_scaled = ml_scaler.transform(features)
            prediction = ml_model.predict(features_scaled)[0]
            confidence = ml_model.predict_proba(features_scaled)[0].max()
            return prediction, confidence
        except:
            # Model not trained yet, use simple heuristic
            if avg_sentiment > 0.1:
                return "BULLISH", 0.6
            elif avg_sentiment < -0.1:
                return "BEARISH", 0.6
            else:
                return "NEUTRAL", 0.5
    except Exception as e:
        print(f"ML prediction error: {e}")
        return None, 0.5

# --- ROUTES ---
@app.route('/')
def index():
    return render_template('dashboard.html')

@app.route('/api/analyze', methods=['POST'])
def analyze():
    ticker = request.json.get('ticker', 'AAPL').upper()
    
    # 1. Fetch Real Stock Data
    price_dates = []
    price_values = []
    current_price = "N/A"
    volume_data = []
    
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="1d", interval="15m")
        
        if not hist.empty:
            price_dates = [d.strftime('%H:%M') for d in hist.index]
            price_values = [round(v, 2) for v in hist['Close'].tolist()]
            volume_data = hist['Volume'].tolist()
            current_price = price_values[-1]
    except Exception as e:
        print(f"‚ö†Ô∏è yFinance Error: {e}")
        price_dates = ["09:30", "10:00", "10:30", "11:00", "11:30", "12:00"]
        price_values = [150, 152, 151, 153, 155, 154]
        volume_data = [1000000] * 6

    # 2. Fetch & Analyze Social Sentiment
    raw_posts = fetch_social_data(ticker)
    
    conn = get_db_connection()
    new_logs = []
    sentiment_scores = []
    
    for text, source in raw_posts:
        score = analyze_sentiment(text)
        sentiment_scores.append(score)
        label = "Positive" if score > 0.05 else "Negative" if score < -0.05 else "Neutral"
        
        # Store in database
        if conn:
            try:
                cur = conn.cursor()
                cur.execute("INSERT INTO logs (ticker, source, text, score, label) VALUES (%s, %s, %s, %s, %s)",
                           (ticker, source, text, score, label))
                conn.commit()
                cur.close()
            except Exception as e:
                print(f"DB insert error: {e}")
        
        new_logs.append({
            'source': source,
            'text': text[:150] + "..." if len(text) > 150 else text,
            'label': label,
            'score': round(score, 3)
        })
    
    if conn:
        conn.close()

    # 3. ML-Based Prediction
    ml_prediction, confidence = get_ml_prediction(ticker, sentiment_scores)
    
    avg_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
    
    if ml_prediction:
        prediction = f"{ml_prediction} üìà" if "BULLISH" in str(ml_prediction) else f"{ml_prediction} üìâ" if "BEARISH" in str(ml_prediction) else f"{ml_prediction} ‚ûñ"
        prediction_details = f"{prediction} (Confidence: {confidence*100:.1f}%)"
    else:
        # Fallback to simple sentiment-based prediction
        prediction = "BULLISH üìà" if avg_score > 0.05 else "BEARISH üìâ" if avg_score < -0.05 else "NEUTRAL ‚ûñ"
        prediction_details = prediction

    # 4. Create sentiment trend that aligns with price data
    sentiment_trend = []
    if sentiment_scores:
        base_sentiment = avg_score
        for i in range(len(price_values)):
            # Add some variation to make the chart interesting
            variation = np.random.normal(0, 0.1)
            sentiment_trend.append(round(base_sentiment + variation, 3))
    else:
        sentiment_trend = [0] * len(price_values)

    return jsonify({
        'ticker': ticker,
        'prediction': prediction_details,
        'avg_score': round(avg_score, 3),
        'current_price': current_price,
        'logs': new_logs[:15],  # Limit to 15 for display
        'total_posts_analyzed': len(new_logs),
        'data_source': 'Real Reddit Data' if USE_REDDIT else 'Simulated Data',
        'nlp_model': 'FinBERT' if USE_FINBERT else 'VADER',
        'chart_data': {
            'labels': price_dates,
            'prices': price_values,
            'sentiment': sentiment_trend,
            'volume': volume_data
        }
    })

@app.route('/api/status', methods=['GET'])
def status():
    """Check system status and capabilities"""
    return jsonify({
        'reddit_connected': USE_REDDIT,
        'finbert_loaded': USE_FINBERT,
        'ml_model_trained': os.path.exists(MODEL_PATH),
        'database_connected': get_db_connection() is not None
    })

# Initialize DB on startup
with app.app_context():
    init_db()

if __name__ == '__main__':
    app.run(debug=True)
